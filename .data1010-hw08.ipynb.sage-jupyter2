{"backend_state":"running","kernel":"julia-1.2","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":658235392},"metadata":{"jupytext":{"formats":"ipynb"},"language_info":{"file_extension":".jl","mimetype":"application/julia","name":"julia","version":"1.2.0"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1572473154709,"exec_count":2,"id":"9a94f2","input":"using Pkg; Pkg.add(\"DecisionTree\")","kernel":"julia-1.2","output":{"0":{"name":"stdout","text":"\u001b[32m\u001b[1m Resolving\u001b[22m\u001b[39m package versions...\n"},"1":{"name":"stdout","text":"\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m `~/.julia/environment/v1.2/Project.toml`\n\u001b[90m [no changes]\u001b[39m\n\u001b[32m\u001b[1m  Updating\u001b[22m\u001b[39m"},"2":{"name":"stdout","text":" `~/.julia/environment/v1.2/Manifest.toml`\n\u001b[90m [no changes]\u001b[39m\n"},"3":{"name":"stdout","output_type":"stream","text":" `~/.julia/environment/v1.2/Manifest.toml`\n \u001b[90m [7806a523]\u001b[39m\u001b[92m + DecisionTree v0.8.4\u001b[39m\n"}},"pos":2,"start":1572473138253,"state":"done","type":"cell"}
{"cell_type":"code","end":1572475182227,"exec_count":18,"id":"230597","input":"using DecisionTree\nfeatures, labels = load_data(\"iris\")\nlabeldict = Dict(map(reverse,enumerate(collect(Set(labels)))))\n\nmodel = DecisionTreeClassifier(max_depth=3)\n\nfor i in 1:100\n    gr()\n    inds = rand(1:length(labels), 50) # subsample training data\n    DecisionTree.fit!(model, features[inds,1:2], labels[inds])\n    scatter(features[:,1], features[:,2], group = labels)\n    heatmap!(4:0.01:8, 2:0.01:5, \n         (x,y) -> labeldict[DecisionTree.predict(model, Any[x y])[1]], \n         fillopacity = 0.2, fillcolor = cgrad([:red, :green, :blue]), colorbar = false)\n    fn = \"heatmap_6_$(i).png\"\n    savefig(fn)\nend","kernel":"julia-1.2","pos":27,"start":1572475160990,"state":"done","type":"cell"}
{"cell_type":"code","end":1572478458306,"exec_count":48,"id":"d174f1","input":"svm_projections()","kernel":"julia-1.2","output":{"0":{"data":{"image/svg+xml":"ebc180c6ee190bf5e84aa034be6929259c966c37"},"exec_count":48}},"pos":16,"start":1572478449585,"state":"done","type":"cell"}
{"cell_type":"code","end":1572480982294,"exec_count":7,"id":"7c37ba","input":"using Random, LinearAlgebra, Statistics, Distributions, Plots, LaTeXStrings, LIBSVM, MLDatasets, DecisionTree\ninclude(\"hw08.jl\");","kernel":"julia-1.2","pos":3,"scrolled":true,"start":1572480982225,"state":"done","type":"cell"}
{"cell_type":"code","end":1572483495427,"exec_count":39,"id":"3e2de7","input":"features, labels = load_MNIST_zeros_and_ones();","kernel":"julia-1.2","pos":18,"start":1572483492998,"state":"done","type":"cell"}
{"cell_type":"code","end":1572484192318,"exec_count":56,"id":"345b1b","input":"using Random; Random.seed!(123)\n\nfunction randblue()\n    t = rand()\n    [t*cos(3t), t*sin(3t)] + 0.1*randn(2)\nend\n\nfunction randred()\n    t = rand()\n    [-0.25, 0.25] + [t*cos(3t), -t*sin(3t)] + 0.1*randn(2)\nend\n\nn = 100\nX = [vcat([randblue()' for _ in 1:n]...); vcat([randred()' for _ in 1:n]...)];\ny = repeat([-1,1], inner = n);\n\nmodel = svmtrain(X', y, kernel = LIBSVM.Kernel.RadialBasis, cost = 100.0)\n\nscatter(X[:,1], X[:,2], legend = false, ratio = 1, group = y, color = [:blue :red])\nheatmap!(-1.5:0.005:1, -0.75:0.005:1, (x,y) -> svmpredict(model, reshape([x y],(2,1)))[1][1], \n         fillopacity = 0.2, fillcolor = cgrad([:blue, :red]))","kernel":"julia-1.2","output":{"0":{"data":{"image/svg+xml":"4c9e96213279b8c2f62abb537e06e7c3c37614a9"},"exec_count":56}},"pos":23,"start":1572484182541,"state":"done","type":"cell"}
{"cell_type":"code","end":1572484413232,"exec_count":61,"id":"216b4b","input":"using JuMP, Ipopt\n\nrbf_kernel(a,b;Œ≥=1) = exp(-Œ≥*sum((a-b).^2));\nlinear_kernel(a,b) = a ‚ãÖ b\nquadratic_kernel(a,b) = (a ‚ãÖ b + 1)^2\n\nmutable struct SVM\n    X::Matrix # training features\n    y::Vector # training labels\n    Œ∑::Union{Vector,Missing}\n    Œ≤::Union{Vector,Missing}\n    Œ±::Union{Real,Missing}\n    C::Real\n    kernel::Function\nend\n\nSVM(X,y;Œ≤=missing,Œ∑=missing,Œ±=missing,C=1.0,kernel=rbf_kernel) = SVM(X,y,Œ≤,Œ∑,Œ±,C,kernel)\n\nfunction fit!(S::SVM)\n    nrows, ncols = size(S.X)\n    model = Model(with_optimizer(Ipopt.Optimizer, print_level=0))\n    @variable(model, Œ∑[1:nrows])\n    @variable(model, Œ±)\n    #TODO: define kernel matrix ùí¶ \n    ùí¶ = [S.kernel(row1,row2) for row1 in eachrow(S.X), row2 in eachrow(S.X)]\n    #TODO: add objective function and constraints \n    @constraint(model, 0 .‚â§ Œ∑ .‚â§ S.C)\n    @constraint(model, S.y ‚ãÖ Œ∑ == 0)\n    @objective(model, Max, -1/2 * (Œ∑ .* y)'*ùí¶*(Œ∑ .* y) + sum(Œ∑))\n    optimize!(model)    \n    S.Œ∑ = value.(Œ∑)\n    #TODO: define S.Œ≤ and S.Œ± \n    S.Œ± = mean((S.y - ùí¶*(S.Œ∑ .* S.y))[0.01 .< S.Œ∑/S.C .< 0.99])\n    nothing\nend\n\nfunction predict(S::SVM, x)\n    #TODO: return prediction vector (don't include signum function)\n    ùí¶Ãátest = [S.kernel(row,x) for row in eachrow(S.X)]\n    ùí¶Ãátest‚ãÖ(S.Œ∑) .* (S.y) .+ S.Œ±\nend\n\nfunction visualize(S::SVM; signum = true)\n    zone(x) = x < -1 ? 1 : (x < 0 ? 2 : (x < 1 ? 3 : 4))\n    xmin, xmax = extrema(S.X[:,1])\n    ymin, ymax = extrema(S.X[:,2])\n    xrange = xmax - xmin\n    xmax += 0.25*xrange\n    xmin -= 0.25*xrange\n    yrange = ymax - ymin\n    ymax += 0.25*yrange\n    ymin -= 0.25*yrange\n    xs = range(xmin, stop = xmax, length=512)\n    ys = range(ymin, stop = ymax, length=512)\n    Plots.heatmap(xs, ys, (x,y) -> (signum ? zone : identity)(predict(S, [x,y])), \n            fillopacity = 0.5, colorbar = false, fontfamily = \"Palatino\", \n            fillcolor = cgrad([:blue, :lightblue, :pink, :red]), aspect_ratio = 1, size = (400, 400))\n    contourplot!(xs, ys, (x,y) -> predict(S, [x,y]), [-1, 0, 1], color = :black, linewidth = 0.5)\n    scatter!(S.X[S.y .== 1, 1], S.X[S.y .== 1, 2], color = :red, label = \"\", ms = 2, opacity = 0.5)\n    scatter!(S.X[S.y .== -1, 1], S.X[S.y .== -1, 2], color = :blue, label = \"\", ms = 2, opacity = 0.5)\nend","kernel":"julia-1.2","output":{"0":{"data":{"text/plain":"visualize (generic function with 1 method)"},"exec_count":61}},"pos":24,"start":1572484413167,"state":"done","type":"cell"}
{"cell_type":"code","end":1572485698210,"exec_count":64,"id":"755dbb","input":"# (d)\nCs = 2.0 .^(-10:10)\nplot(-10:10,\n    [mean(count_support_vectors(num_runs=100, sample_size=200, dim=20, Œº=1, C=C)[1]) for C in Cs],\n    label = L\"\\mu = 1\", xlabel = \"log2(C)\", ylabel = \"number of support vectors\", fontfamily = \"Palatino\",\n    ylims = (0,200))\nplot!(-10:10,\n    [mean(count_support_vectors(num_runs=100, sample_size=200, dim=20, Œº=5/9, C=C)[1]) for C in Cs],\n    label = L\"\\mu = 1\", xlabel = \"log2(C)\", ylabel = \"number of support vectors\", fontfamily = \"Palatino\",\n    ylims = (0,200))\n\nplot!(-10:10,\n    [mean(count_support_vectors(num_runs=100, sample_size=200, dim=20, Œº=0, C=C)[1]) for C in Cs],\n    label = L\"\\mu = 1\", xlabel = \"log2(C)\", ylabel = \"number of support vectors\", fontfamily = \"Palatino\",\n    ylims = (0,200))\n\nplot!(-10:10,\n    [mean(count_support_vectors(num_runs=100, sample_size=200, dim=20, Œº=2, C=C)[1]) for C in Cs],\n    label = L\"\\mu = 1\", xlabel = \"log2(C)\", ylabel = \"number of support vectors\", fontfamily = \"Palatino\",\n    ylims = (0,200))","kernel":"julia-1.2","output":{"0":{"ename":"InterruptException","evalue":"InterruptException:","traceback":["InterruptException:","","Stacktrace:"," [1] #svmtrain#3(::Type, ::LIBSVM.Kernel.KERNEL, ::Int64, ::Float64, ::Float64, ::Float64, ::Float64, ::Float64, ::Float64, ::Bool, ::Bool, ::Nothing, ::Float64, ::Bool, ::Int64, ::typeof(svmtrain), ::Adjoint{Float64,Array{Float64,2}}, ::Array{Int64,1}) at /home/user/.julia/packages/LIBSVM/jzCVO/src/LIBSVM.jl:386"," [2] #svmtrain at ./none:0 [inlined]"," [3] #simulate_and_train#27(::Int64, ::Int64, ::Float64, ::LIBSVM.Kernel.KERNEL, ::Float64, ::typeof(simulate_and_train)) at /home/user/hw08/hw08.jl:34"," [4] #simulate_and_train at ./array.jl:0 [inlined]"," [5] #count_support_vectors#28(::Int64, ::Int64, ::Int64, ::Float64, ::Float64, ::typeof(count_support_vectors)) at /home/user/hw08/hw08.jl:55"," [6] #count_support_vectors at ./none:0 [inlined]"," [7] (::getfield(Main, Symbol(\"##131#132\")))(::Float64) at ./none:0"," [8] iterate at ./generator.jl:47 [inlined]"," [9] collect_to!(::Array{Float64,1}, ::Base.Generator{Array{Float64,1},getfield(Main, Symbol(\"##131#132\"))}, ::Int64, ::Int64) at ./array.jl:651"," [10] collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{Array{Float64,1},getfield(Main, Symbol(\"##131#132\"))}, ::Int64) at ./array.jl:630"," [11] collect(::Base.Generator{Array{Float64,1},getfield(Main, Symbol(\"##131#132\"))}) at ./array.jl:611"," [12] top-level scope at In[64]:7"]}},"pos":14,"start":1572485548593,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5609d6","input":"","pos":21,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":4,"id":"c29e25","input":"minimax_example()","output":{"0":{"data":{"image/svg+xml":"7d05577e3950912f38e1c9a217b1a24407df09ee"},"exec_count":4,"output_type":"execute_result"}},"pos":8,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":4,"id":"f62a8e","input":"lagrange_example()","output":{"0":{"data":{"image/svg+xml":"56d2ceb172acd65664ed69f28ffe63fd8d729310"},"exec_count":4,"output_type":"execute_result"}},"pos":5,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":5,"id":"224f51","input":"# the value 5/9 is chosen so that separated and non-separated examples are both well represented\nSV_counts, sep_values = count_support_vectors(num_runs=100, sample_size=100, dim=20, Œº=5/9)","output":{"0":{"data":{"text/plain":"([19, 20, 30, 21, 35, 29, 27, 21, 25, 27  ‚Ä¶  23, 31, 28, 21, 21, 19, 20, 24, 43, 32], Bool[1, 1, 0, 1, 0, 0, 0, 1, 0, 0  ‚Ä¶  0, 0, 0, 1, 1, 1, 1, 0, 0, 0])"},"exec_count":5,"output_type":"execute_result"}},"pos":12,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":6,"id":"32470a","input":"# (b)\nX, y, model = simulate_and_train(sample_size=1000, dim=20, Œº=0.58)\nŒ≤ = -model.SVs.X * model.coefs\nŒ≤ / norm(Œ≤)","output":{"0":{"data":{"text/plain":"20√ó1 Array{Float64,2}:\n 0.2074422478956568 \n 0.25839875840538984\n 0.18361970588009235\n 0.24144883908098488\n 0.12324474704542134\n 0.1782741417347018 \n 0.1227870259198149 \n 0.2523695882014227 \n 0.27855229365108863\n 0.20334828446328923\n 0.22216006640473338\n 0.24514815736237   \n 0.1825875413529991 \n 0.25764068960142616\n 0.22890628294997392\n 0.25884836103378983\n 0.1626527458158216 \n 0.22197847790976963\n 0.23188412397638086\n 0.3089228624250256 "},"exec_count":6,"output_type":"execute_result"}},"pos":13,"state":"done","type":"cell"}
{"cell_type":"code","id":"978cdf","input":"","pos":28,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"054746","input":"*Solution*. ","pos":26,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"0f35cb","input":"---\n\n## Problem 3\n\nUse the code below to simulate a dataset with two classes, one of which is Gaussian with mean zero and covariance $I$ in $\\mathbb{R}^p$, while the other is Gaussian with mean $\\mu\\mathbf{1}$ and covariance $I$ (note: $\\mathbf{1}$ represents a vector of ones in $\\mathbb{R}^p$. Explore the following questions empirically. \n\n(a) Is there a difference between the distribution of the number of support vectors in linearly separable instances and the number of support vectors in instances which are not separable? Note that you can make two histograms at once by supplying a vector to use for grouping to the `group` keyword argument.  \n(b) What should the direction of $\\boldsymbol{\\beta}$ be, given the way in which the data were generated? (In other words, the decision boundary for the Bayes classifier is actually a hyperplane; what unit vector $\\boldsymbol{\\beta}$ is perpendicular to that hyperplane?) Does $\\boldsymbol{\\beta}$ come out to be very close to this value?  \n(c) How does the number of support vectors vary with dimension? Choose a $\\mu$ value which is large enough to ensure that the classes are linearly separated with high probability.  \n(d) How does the number of support vectors vary with $C$? Try a few $\\mu$ values. ","pos":10,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"18ccb2","input":"*Solution*. ","pos":11,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4118be","input":"### Solution to Problem 1\n\nThe objective function is given by $L(x,y,\\gamma) = x^2 + y^2 + \\lambda(-x-y+5)$. The corresponding minmax problem is $\\min_{x,y}\\max_{\\lambda} L(x,y,\\gamma)$ and the dual is $\\max_{\\lambda}\\min_{x,y} L(x,y,\\gamma)$\n\nDoing them separately:\n\n$$\n\\begin{align}\n\\min_{x,y}\\max_{\\lambda} x^2 + y^2 + \\lambda(-x-y+5)\\\\\n\\text{the outer minimisation works when}\\\\\n-x-y+5 &= 0\\\\\ny &= 5 - x\n\\min_{x,y} x^2 + (5-x)^2\\\\\n\\text{Since COCalc is EXTREMELY SLOW, not showing any work}\\\\\ny &= \\frac{5}{2}\\\\\nx &= \\frac{5}{2}\n\\end{align}\n$$\n\n$$\n\\begin{align}\n\\max_{\\lambda}\\min_{x,y} x^2 + y^2 + \\lambda(-x-y+5)\\\\\n\\text{the inner minimisation gives}\\\\\nx &= y\\\\\n\\text{therefore the outer maximisation gives}\\\\\nk &= \\frac{5}{2} = x = y\n\\end{align}\n$$\nTherefore we find the same solution to the objective function","pos":6,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"4c2623","input":"---\n\n## Problem 5\n\nUse LIBSVM to train an SVM classifier on the Enron email data from Homework 07 (using the same scheme to encode the email messages as vectors). Try a linear classifier as well as one trained with radial basis functions, and select your value for $C$ using cross-validation on a withheld subset of the training data. Test your classifier on one of the folders which was not used for training‚Äìwhich performs better?","pos":19,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"67a63f","input":"---\n\n## Problem 6\n\nThe kernelized soft-margin SVM turns out to boil down to a story eerily reminiscient of kernel density estimation: at each training observation, we spread out a *charge* in the shape of the chosen kernel (charge, now, instead of mass, since it can carry a positive or negative sign) centered at each training observation. Given a point $\\mathbf{x}$ to make a prediction for, we sum the amounts of charge contributed at $\\mathbf{x}$ from each of the training points, and just work out whether the resulting sum is positive or negative. \n\nThe way in which the charges are distributed has some constraints: the amount of charge associated with each training point is never more than $C$ and never has the opposite sign to the class of the training point. Also, we have to put the same total amount of charge on the positive and negative training examples.\n\nWe could build a machine learning model which is more similar to kernel density estimation: each training point gets the same total amount of charge (positive it's positive, negative otherwise). Such a model would be significantly less effective than the kernelized soft-margin SVM on real-world data. Describe what you think its main advantages are. \n\nHint: starting with the code below, make a plot which shows the support vectors for the radial-basis SVM (with high cost value), for the given set of curvily entangled data.","pos":22,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"91364b","input":"---\n\n## Problem 4\n\nUse LIBSVM to project the zeros and ones in the MNIST training set to the orthogonal complement of the vector $\\boldsymbol{\\beta}$ obtained by training a linear SVM on the original data. Are these new data linearly separable? What does your low-dimensional intuition suggest?","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"a0743a","input":"## Homework 08\n\n#### Due: *01 November 2019*\n#### *DATA 1010*","pos":1,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b1c1b6","input":"*Solution*. ","pos":9,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"b4906b","input":"*Solution*. ","pos":17,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"cb0de9","input":"*Solution*. ","pos":20,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d40c00","input":"---\n\n## Problem 7\n\nTrain 100 `DecisionTreeClassifier`s on a randomly selected sample of size 50 chosen from the Iris training observations. Just use the first two features, so you can plot a heatmap. How volatile are the predictions from one such tree to another? Consider making several heatmap images and displaying them in a grid, like so:\n```julia\np = [scatter(randn(20), randn(20), label = \"\") for _ in 1:12] # a dozen random plots\nplot(p..., layout = (4,3), size = (1000,600))\n```\nHow volatile are the predictions made by plurality vote of the ensemble of 100 classifiers? Describe qualitatively.","pos":25,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fd1c80","input":"---\n\n## Problem 1\n\nApply the Lagrange duality method we applied in class to the soft-margin SVM to the problem of minimizing the objective function $f(x,y) = x^2 + y^2$ subject to the constraint $x + y \\ge 5$. Show that strong duality holds (that is, we get the same value for the objective function if we swap min and max).","pos":4,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"fef5a0","input":"---\n\n## Problem 2\n\nShow that the minimax inequality sometimes holds strictly. In other words, find a function $(x,y) \\mapsto f(x,y)$ such that $\\min_{x}\\max_{y} f(x,y) > \\max_{x}\\min_{y} f(x,y)$. \n\nHint: consider functions $f$ on the unit square which are constant on the regions shown in blue and orange below. \n\nExplain in loose terms why you would not expect this kind of behavior to emerge when $f$ is a convex function on the unit square.","pos":7,"state":"done","type":"cell"}
{"end":1572483454910,"exec_count":36,"id":"9e1eb5","input":"p = scatter(features' * Œ≤c,  zeros(size(features,2)), group = labels, legend = :bottomright, size = (450,300), ratio = 1)","kernel":"julia-1.2","output":{"0":{"data":{"image/svg+xml":"42db5ab723847eb4e84edd7b34006a16f7e45829"},"exec_count":36}},"pos":18.75,"start":1572483454039,"state":"done","type":"cell"}
{"end":1572483499792,"exec_count":40,"id":"4ab56c","input":"model = svmtrain(float(features), float(labels), kernel = LIBSVM.Kernel.Linear, cost=100.0)\nŒ≤ = model.SVs.X * model.coefs\nŒ≤c = nullspace(Matrix(Œ≤'))[:,1];","kernel":"julia-1.2","pos":18.5,"start":1572483495480,"state":"done","type":"cell"}
{"end":1572483517934,"exec_count":41,"id":"4687d1","input":"p = scatter(features' * Œ≤,  zeros(size(features,2)), group = labels, legend = :bottomright, size = (450,300), ratio = 1)","kernel":"julia-1.2","output":{"0":{"data":{"image/svg+xml":"d7c242c939315723e09590a57aa6335fd93ba351"},"exec_count":41}},"pos":18.875,"start":1572483516748,"state":"done","type":"cell"}
{"end":1572483861040,"exec_count":50,"id":"7146df","input":"model.SVs.X","kernel":"julia-1.2","output":{"0":{"data":{"text/plain":"2√ó45 Array{Float64,2}:\n  0.0691787  -0.179557  0.140099  ‚Ä¶  0.102318  -0.312717  0.00194366\n -0.0494581   0.475352  0.136191     0.155224   0.362182  0.15343   "},"exec_count":50}},"pos":23.25,"start":1572483861035,"state":"done","type":"cell"}
{"end":1572483865002,"exec_count":51,"id":"0da3d7","input":"plot!(model.SVs.X[1,:],model.SVs.X[2,:])","kernel":"julia-1.2","output":{"0":{"data":{"image/svg+xml":"c4d00c8bfb71658423974673a8eefc74ac18f500"},"exec_count":51}},"pos":23.5,"start":1572483863687,"state":"done","type":"cell"}
{"end":1572484542920,"exec_count":63,"id":"d900bc","input":"S = SVM(X, y, kernel=rbf_kernel, C = 1.0)\nfit!(S)\nvisualize(S)","kernel":"julia-1.2","output":{"0":{"ename":"MethodError","evalue":"MethodError: no method matching isless(::Array{Float64,1}, ::Int64)\nClosest candidates are:\n  isless(!Matched::Missing, ::Any) at missing.jl:66\n  isless(!Matched::AbstractFloat, ::Real) at operators.jl:158\n  isless(!Matched::ForwardDiff.Dual{Tx,V,N} where N where V, ::Integer) where Tx at /ext/julia/depot/packages/ForwardDiff/N0wMF/src/dual.jl:139\n  ...","traceback":["MethodError: no method matching isless(::Array{Float64,1}, ::Int64)\nClosest candidates are:\n  isless(!Matched::Missing, ::Any) at missing.jl:66\n  isless(!Matched::AbstractFloat, ::Real) at operators.jl:158\n  isless(!Matched::ForwardDiff.Dual{Tx,V,N} where N where V, ::Integer) where Tx at /ext/julia/depot/packages/ForwardDiff/N0wMF/src/dual.jl:139\n  ...","","Stacktrace:"," [1] <(::Array{Float64,1}, ::Int64) at ./operators.jl:268"," [2] (::getfield(Main, Symbol(\"#zone#126\")))(::Array{Float64,1}) at ./In[61]:44"," [3] (::getfield(Main, Symbol(\"##124#127\")){Bool,SVM,getfield(Main, Symbol(\"#zone#126\"))})(::Float64, ::Float64) at ./In[61]:55"," [4] Surface(::getfield(Main, Symbol(\"##124#127\")){Bool,SVM,getfield(Main, Symbol(\"#zone#126\"))}, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}) at /ext/julia/depot/packages/Plots/h3o4c/src/components.jl:658"," [5] macro expansion at /ext/julia/depot/packages/Plots/h3o4c/src/series.jl:462 [inlined]"," [6] apply_recipe(::Dict{Symbol,Any}, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::Function) at /ext/julia/depot/packages/RecipesBase/zBoFG/src/RecipesBase.jl:275"," [7] _process_userrecipes(::Plots.Plot{Plots.GRBackend}, ::Dict{Symbol,Any}, ::Tuple{StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}},StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}},getfield(Main, Symbol(\"##124#127\")){Bool,SVM,getfield(Main, Symbol(\"#zone#126\"))}}) at /ext/julia/depot/packages/Plots/h3o4c/src/pipeline.jl:83"," [8] _plot!(::Plots.Plot{Plots.GRBackend}, ::Dict{Symbol,Any}, ::Tuple{StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}},StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}},getfield(Main, Symbol(\"##124#127\")){Bool,SVM,getfield(Main, Symbol(\"#zone#126\"))}}) at /ext/julia/depot/packages/Plots/h3o4c/src/plot.jl:178"," [9] #plot#133(::Base.Iterators.Pairs{Symbol,Any,NTuple{7,Symbol},NamedTuple{(:fillopacity, :colorbar, :fontfamily, :fillcolor, :aspect_ratio, :size, :seriestype),Tuple{Float64,Bool,String,ColorGradient,Int64,Tuple{Int64,Int64},Symbol}}}, ::typeof(plot), ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::Vararg{Any,N} where N) at /ext/julia/depot/packages/Plots/h3o4c/src/plot.jl:57"," [10] (::getfield(RecipesBase, Symbol(\"#kw##plot\")))(::NamedTuple{(:fillopacity, :colorbar, :fontfamily, :fillcolor, :aspect_ratio, :size, :seriestype),Tuple{Float64,Bool,String,ColorGradient,Int64,Tuple{Int64,Int64},Symbol}}, ::typeof(plot), ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::Vararg{Any,N} where N) at ./none:0"," [11] #heatmap#394(::Base.Iterators.Pairs{Symbol,Any,NTuple{6,Symbol},NamedTuple{(:fillopacity, :colorbar, :fontfamily, :fillcolor, :aspect_ratio, :size),Tuple{Float64,Bool,String,ColorGradient,Int64,Tuple{Int64,Int64}}}}, ::typeof(heatmap), ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::Vararg{Any,N} where N) at /ext/julia/depot/packages/RecipesBase/zBoFG/src/RecipesBase.jl:369"," [12] (::getfield(Plots, Symbol(\"#kw##heatmap\")))(::NamedTuple{(:fillopacity, :colorbar, :fontfamily, :fillcolor, :aspect_ratio, :size),Tuple{Float64,Bool,String,ColorGradient,Int64,Tuple{Int64,Int64}}}, ::typeof(heatmap), ::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}}, ::Vararg{Any,N} where N) at ./none:0"," [13] #visualize#123(::Bool, ::typeof(visualize), ::SVM) at ./In[61]:55"," [14] visualize(::SVM) at ./In[61]:44"," [15] top-level scope at In[63]:3"]}},"pos":24.5,"start":1572484542474,"state":"done","type":"cell"}
{"id":"7ff151","input":"","pos":29,"state":"done","type":"cell"}
{"id":0,"time":1572487957811,"type":"user"}
{"last_load":1572455130085,"type":"file"}